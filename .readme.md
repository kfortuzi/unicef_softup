### Deploying infrastructure

1. In AWS EC2 create 2 key pairs, akpa-ai-dev.pem and akpa-ai-prod.pem
2. In Repository variables at first we need these variables:
   - AWS_ACCESS_KEY_ID
   - AWS_SECRET_ACCESS_KEY
   - SSH_KEY_DEV (the base 64 content of the pem file)
   - SSH_KEY_PROD
   - OPEN_AI_KEY_DEV (We are using 1 api key for both environments)
3. Run the pipeline to deploy the infrastructure steps.
4. Pause the pipeline, go to AWS EC2 and copy the server public ip.
5. Add another repository variable:
   - SERVER_IP_DEV
6. Resume the pipeline
7. Repeat steps 3, 4, 5, 6 for the production environment

### Setting up the secrets in AWS Secrets Manager

In order for the pipeline build to succedd we have to manually add these entries in SM and rerun the pipeline after.

*To generate random secrets: node > crypto.randomBytes(16).toString('base64')*

1. [dev and prod]/strapi
```json
{
  strapiJwtSecret: "base64",
  strapiAdminJwtSecret: "base64",
  strapiAppKeys: "base64,base64",
  strapiApiTokenSalt: "base64",
  strapiTransferTokenSalt: "base64"
}
```
2. [dev and prod]/strapi/api (this will handle env variables used in the nest api)
```json
{
  strapiToken: "The token generated in strapi admin panel",
  apiJwtSecret: "crypto.randomBytes(32).toString('base64')",
  openAiApiKey: "The api key from openai dashboard"
}
```
3. akpa/pinecone (the pinecone database secrets, 1 for both envs atm)
```json
{
  pineconeKey: "string",
  pineconeIndex: "string"
}
```

### Setting up Strapi database

1. After RDS has been deployed, ssh into the api server instance
2. Install pg client `sudo apt-get install postgresql-client` (it might require also another command to install a specific client version, copy the command it says and execute it)
3. Run `psql --host=[rds-host] --port=5432 --username=master_user --password --dbname=akpa_ai_rds_[dev or prod]`
4. In the psql console run `CREATE DATABASE strapi_db_[dev or prod];`
5. To access strapi database, use the same command in step 3, by changing only the database name

### Running the project locally

1. Create a .env file
```
DATABASE_URL="postgresql://user:postgres@localhost:5432/akpa_boilerplate?schema=public"
SERVER_PORT=3000
FE_HOST="http://localhost:5173"
AWS_S3_BUCKET_NAME=""
BASE_JOB_URL="https://www.puna.gov.al/shkp_api/odoo/getJob/"
FEATURED_JOBS_URL="https://www.puna.gov.al/shkp_api/odoo_post/getFeaturedJobs/5000/1"
STRAPI_BASE_URL="http://localhost:1337/api"
STRAPI_TOKEN="token"
AWS_REGION="eu-central-1"
OPENAI_API_KEY=""
PINECONE_KEY=""
PINECONE_INDEX=""
API_JWT_SECRET=""
```
2. Spinning up the backend `cd nest-rest-api && docker-compose up -f docker-compose.dev.yaml up -d --build`
3. Run prisma migrations `npm run prisma:generate && npm run prisma:migrate`
4. Spinning up the frontend `cd react-app && npm run dev`
5. If you want to reflect the real time changes in the api, stop the api container by keeping only the database and run `npm run start:dev`
6. To start the strapi stack `cd asset-api && docker-compose up -d`. For the local env variables check `asset-api/.env.example`

### The env file for production
The env files for production are created from `deployment/env.sh` script and uploaded to the server instance so the docker stack can read and use them. Everytime when we introduce a new variable we should add it in Secrets Manager and read it in `deployment/env.sh`.